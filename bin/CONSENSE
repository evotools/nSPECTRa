#!/usr/bin/env python
from collections import Counter
import logging
import os

import polars as pl
from pysam import FastaFile


def parse_args():
    """Define arguments."""
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-b",
        "--bed",
        metavar="ancestral_bed.bed",
        type=str,
        help="Input ancestral bed file",
        dest="bed",
        required=True,
    )
    parser.add_argument(
        "-f",
        "--fasta",
        metavar="reference.fasta",
        type=str,
        help="Input fasta file",
        dest="fasta",
        required=True,
    )
    parser.add_argument(
        "-r",
        "--region",
        metavar="str",
        type=str,
        help="Sequence ID to process",
        dest="region",
        required=True,
    )
    parser.add_argument(
        "-t",
        "--threads",
        metavar="N",
        type=int,
        help="Number of threads; defaults all of them",
        dest="threads",
        required=False,
    )
    parser.add_argument(
        "-g", "--greedy", action="store_true", help="Run in greedy mode.", dest="greedy"
    )
    parser.add_argument(
        "-o",
        "--out",
        metavar="prefix",
        type=str,
        help="Output file.",
        dest="out",
        required=False,
        default="myanc.fa",
    )
    return parser.parse_args()


# Define consensus allele
def consense_query(input_alleles):
    input_alleles = input_alleles.upper().replace("N", "-").replace(".", "-")
    ref = input_alleles[0]
    cnt_full = Counter(input_alleles)
    cnt_alts = Counter(input_alleles[1:])
    candidate = cnt_alts.most_common()
    second = cnt_full.most_common()
    # Refine logic
    if ref == "-" or list(cnt_alts.keys()) == ["-"] or list(cnt_full.keys()) == ["-"]:
        return "-"
    if len(candidate) == 1:
        return candidate[0][0]
    if len(candidate) >= 2 and candidate[0][1] > candidate[1][1]:
        return candidate[0][0] if len(candidate) == 2 else candidate[0][0].lower()
    if len(candidate) >= 2 and second[0][1] > second[1][1]:
        return second[0][0].lower()
    return "-"


# Main function
def main():
    # Import arguments
    args = parse_args()

    # Define logger
    global _log_name
    _log_name = 'CONSENSE'
    logging.basicConfig(
        format="[%(asctime)s - %(name)s] %(message)s",
        datefmt="%H:%M:%S",
        level=logging.INFO,
    )
    logger = logging.getLogger(_log_name)

    # Define thread pool
    if args.threads:
        logger.info(f"Setting threads to: {args.threads}")
        os.environ["POLARS_MAX_THREADS"] = f"{args.threads}"

    # Instatiate reference
    reference = FastaFile(args.fasta)

    # Load the BED file
    logging.info("Loading bed")
    df = (
        pl.read_csv(
            args.bed,
            has_header=False,
            separator="\t",
            new_columns=["chrom", "start", "end", "val"],
        )
        .cast({"chrom": str, "start": int, "end": int, "val": str})
        .filter(pl.col("chrom") == args.region)
    )

    # Verticalize the dataset
    logging.info("Verticalizing")
    df = (
        df.with_columns(
            val=pl.col("val").str.split(by=""),
            start=pl.int_ranges("start", "end"),
            end=pl.int_ranges("start", "end"),
        )
        .explode(["start", "end", "val"])
        .filter(pl.col("val") != "-")
        .sort(by=("chrom", "start"))
        .with_columns(end=pl.col("end") + 1)
    )

    # Combine bases for same position
    if args.greedy:
        logging.info("Grouping data (greedy)")
        df = (
            df
            .lazy()
            .group_by(("chrom", "start", "end"), maintain_order=True)
            .agg(pl.col("val").str.concat(""))
            .collect()
        )
    else:
        logging.info("Grouping data (memory)")
        df = (
            df
            .lazy()
            .group_by(("chrom", "start", "end"), maintain_order=True)
            .agg(pl.col("val"))
            .with_columns(
                val=pl.col("val").map_elements(
                    lambda x: "".join(x), return_dtype=str, strategy="threading"
                )
            )
            .collect()
        )

    # Sort positions
    logging.info("Sorting")
    df = df.sort(by=("chrom", "start"))

    # Add reference bases
    logging.info("Add reference")
    reference = pl.DataFrame(
        {
            "chrom": args.region,
            "start": range(0, len(reference[args.region])),
            "ref": tuple(reference[args.region]),
        }
    )
    df = df.join(reference, on=("chrom", "start"), how="left")

    # Add reference base, then drop the ref col
    df = df.with_columns(val=pl.concat_str(pl.col("ref"), pl.col("val"))).drop("ref")

    # Get consensed value
    logging.info("Run consensus")
    df = df.with_columns(
        con=pl.col("val").map_elements(
            lambda x: consense_query(x), return_dtype=str, strategy="threading"
        )
    )

    # Save data
    df.write_csv(args.out, separator="\t", include_header=False)
    logging.info("Done")
    return 0


if __name__ == "__main__":
    try:
        main()
    except:
        raise MemoryError("Out of memory")
